{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO 3350/6350\n",
    "\n",
    "## Lecture 06: Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection\n",
    "\n",
    "We've spoken before about **overfitting**. A model that is overfit has learned features of the training data that are unlikely to generalize to new, unseen data. As a rule of thumb, a model is very likely to be overfit if it uses on the order of as many features as the training data contains examples. The signature of an overfit model is good performance when evaluated against the training data, but poor performance when evaluated against held-out test data. \n",
    "\n",
    "We also know that models fit to high-dimensional datasets can be difficult to interpret. If we use fewer features (while maintaining good performance), it will generally be easier to explain our system.\n",
    "\n",
    "> **Discuss: How might you go about selecting the best features for a classification task?**\n",
    "\n",
    "Mathematical dimension reduction is one approach to addressing high dimensionality and overfitting. Another is **feature selection**. When we select features to retain in our model, we're looking for ones that seem likely to provide useful information about the relationship between our objects and the class labels we seek to assign. Two useful approaches to feature selection are **ANOVA f-statistics** and **mutual information**. \n",
    "\n",
    "We'll look at both of these methods to estimate feature importance, then see how to use them in `sklearn` to pare down a feature set. We'll also mention **permutation importance**, a strictly *post hoc* empirical measure of feature importance based on the impact to a specified system of ablating individual features.\n",
    "\n",
    "### *F*-statistic and ANOVA analysis\n",
    "\n",
    "We won't say much more about ANOVA analysis; you should have seen it in your stats class. It's conceptually similar to a *t*-test, in that it compares the difference in the means of two groups relative to the variance within each group. A large *F*-statistic means that two samples are significantly different. For feature selection, we want features that are (significantly) differently distributed between the classes.\n",
    "\n",
    "The problem with the *F*-statistic is that it only works well for linear, normally distributed relationships. The advantages are (1) that this assumption is often valid and (2), it's super easy to calculate, so is fast when run on large datasets.\n",
    "\n",
    "### Mutual information\n",
    "\n",
    "**Mutual information** (MI) is abstractly similar to correlation or covariance -- it's a measure of how much two variables change together. But MI doesn't assume a linear relationship between the variables and it's suitable for categorical data, so is often preferred for our purposes.\n",
    "\n",
    "If you want the math, in the case of categorical data (like we're using here with word counts and class labels), mutual information is calculated as:\n",
    "\n",
    "$$MI(U,V)=\\sum_{i=1}^{|U|} \\sum_{j=1}^{|V|}\\frac{|U_i\\cap V_j|}{N}\\log\\frac{N|U_i \\cap V_j|}{|U_i||V_j|}$$\n",
    "\n",
    "Where $\\cap$ indicates the **insection** of two sets. $N$ is the total number of observations in each vector. $i$ and $j$ represent the possible values in each vector. And $|U_i|$ is the count of instances of value $i$ in vector $U$.\n",
    "\n",
    "If we have two vectors, $U = [\\alpha, \\beta]$ and $V = [\\alpha, \\beta]$, we know that our MI score should be high, since $U$ is identical to $V$. For an *n*-class problem, the maximum unnormalized MI is log(*n*). In our example, $N$ = 2 (two observations per vector) and $i$ and $j$ are both drawn from the set $\\{\\alpha,\\beta\\}$. Hence:\n",
    "\n",
    "$$MI(U,V) = \\frac{1}{2}\\log\\frac{2(1)}{1(1)} + 0 + 0 + \\frac{1}{2}\\log\\frac{2(1)}{1(1)} = 0.693147$$\n",
    "\n",
    "The zeros correspond in this example to misaligned labels, of which there are none, hence $|U_i\\cap V_j| = 0$ for all $i\\neq j$ (again, in this specific example of perfect label alignment, not in all cases).\n",
    "\n",
    "Or consider the slightly more interesting case where $U = [1,1,2]$ and $V = [1,2,2]$. In this case, we calculate:\n",
    "\n",
    "$$MI(U,V) = \\frac{1}{3}\\log\\frac{3(1)}{2(1)} + \\frac{1}{3}\\log\\frac{3(1)}{2(2)} + 0 + \\frac{1}{3}\\log\\frac{3(1)}{1(2)} = 0.174416$$\n",
    "\n",
    "Mutual information is often (but not always) reported on a *normalized* basis, in which the raw MI value is divided by log($N$) (or, technically, by the mean entropy of the two variables, but it amounts to the same thing when there are the same number of classes in each variable), the maximum possible MI value for the case. The normalized MI for the first example is 0.693147/log(2) = 1.0. In the second example, normalized MI = 0.174416/log(3) = 0.15876.\n",
    "\n",
    "See the [sklearn documentation for `mutual_info_classif`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_selection import mutual_info_classif, f_classif, SelectKBest\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm hand calculations from above\n",
    "\n",
    "assert np.isclose(\n",
    "    mutual_info_classif(\n",
    "        np.array([[1,0]]).T, # transpose to column vector\n",
    "        np.array([1,0]),\n",
    "        discrete_features=True),\n",
    "    0.693147\n",
    ")\n",
    "assert np.isclose(\n",
    "    mutual_info_classif(\n",
    "        np.array([[1,1,2]]).T, # ditto\n",
    "        np.array([1,2,2]),\n",
    "        discrete_features=True),\n",
    "    0.174416\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo on trivial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f statistic: [4.  0.5]\n",
      "p values: [0.11611652 0.51851852]\n"
     ]
    }
   ],
   "source": [
    "# f-stat\n",
    "X = np.array([[1,1,1,1,0,0], [1,0,1,0,1,0]]).T\n",
    "y = np.array([1,1,1,0,0,0])\n",
    "f_stat, p_vals = f_classif(X,y)\n",
    "print(\"f statistic:\", f_stat)\n",
    "print(\"p values:\", p_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# automated selection\n",
    "SelectKBest(k=1, score_func=f_classif).fit_transform(X,y) # note score_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.31825708, 0.05663301])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mutual info\n",
    "mutual_info_classif(X, y, discrete_features=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A small corpus\n",
    "\n",
    "About 125,000 news articles, distributed evenly across four categories (world, business, sports, and science/technology), each trimmed to contain **just the first sentence or two of the original article**. [Data source](https://github.com/mhjabreel/CharCnn_Keras/tree/master/data/ag_news_csv), plus minor massaging into current format.\n",
    "\n",
    "Our task will be to predict the category of any news article using a Bayesian classifier, and to examine the impact of feature selection on model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and clean news data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sci_Tech</td>\n",
       "      <td>Mobile phone giant Nokia has unveiled its firs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sports</td>\n",
       "      <td>ADELAIDE, Nov 30: Australia crushed New Zealan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sports</td>\n",
       "      <td>Flinching in the trenches, holding around quar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sports</td>\n",
       "      <td>Miami Dolphins owner Wayne Huizenga and presid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sci_Tech</td>\n",
       "      <td>InfoWorld - PeopleSoft's Board of Directors vo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sci_Tech</td>\n",
       "      <td>AP - Edward Bitet fought in World War II, buil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Business</td>\n",
       "      <td>LARRY Ellison, the chief executive of software...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sci_Tech</td>\n",
       "      <td>AP - Indonesian police on Tuesday summoned the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>World</td>\n",
       "      <td>AP - Police defused a time-bomb in a town near...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>World</td>\n",
       "      <td>A full season has passed since the UN Security...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Sci_Tech</td>\n",
       "      <td>November 03, 2004 - The addictive brutality an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Sports</td>\n",
       "      <td>AP - Gerald Williams and Eric Valent hit solo ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>World</td>\n",
       "      <td>The Palestinians have taken a double hit this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Sci_Tech</td>\n",
       "      <td>Hitachi Global Storage Technologies Inc. and I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Sci_Tech</td>\n",
       "      <td>The new desktops, priced between \\$600 and \\$7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Sports</td>\n",
       "      <td>by Paul Upham: It was a fight that was to rece...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Business</td>\n",
       "      <td>NEW YORK (Reuters) - Kraft Foods Inc. &amp;lt;A H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Sports</td>\n",
       "      <td>AP - Jordan Palmer threw a 25-yard touchdown p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Sci_Tech</td>\n",
       "      <td>The Open Source Development Labs has gone into...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Sports</td>\n",
       "      <td>AP - Jeff Brehaut shot an 11-under 61 Friday a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                               body\n",
       "0   Sci_Tech  Mobile phone giant Nokia has unveiled its firs...\n",
       "1     Sports  ADELAIDE, Nov 30: Australia crushed New Zealan...\n",
       "2     Sports  Flinching in the trenches, holding around quar...\n",
       "3     Sports  Miami Dolphins owner Wayne Huizenga and presid...\n",
       "4   Sci_Tech  InfoWorld - PeopleSoft's Board of Directors vo...\n",
       "5   Sci_Tech  AP - Edward Bitet fought in World War II, buil...\n",
       "6   Business  LARRY Ellison, the chief executive of software...\n",
       "7   Sci_Tech  AP - Indonesian police on Tuesday summoned the...\n",
       "8      World  AP - Police defused a time-bomb in a town near...\n",
       "9      World  A full season has passed since the UN Security...\n",
       "10  Sci_Tech  November 03, 2004 - The addictive brutality an...\n",
       "11    Sports  AP - Gerald Williams and Eric Valent hit solo ...\n",
       "12     World  The Palestinians have taken a double hit this ...\n",
       "13  Sci_Tech  Hitachi Global Storage Technologies Inc. and I...\n",
       "14  Sci_Tech  The new desktops, priced between \\$600 and \\$7...\n",
       "15    Sports  by Paul Upham: It was a fight that was to rece...\n",
       "16  Business   NEW YORK (Reuters) - Kraft Foods Inc. &lt;A H...\n",
       "17    Sports  AP - Jordan Palmer threw a 25-yard touchdown p...\n",
       "18  Sci_Tech  The Open Source Development Labs has gone into...\n",
       "19    Sports  AP - Jeff Brehaut shot an 11-under 61 Friday a..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data from disk and examine\n",
    "news = pd.read_csv(os.path.join('..', 'data', 'news', 'news_text.csv.gz'))\n",
    "news.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I bet those datelines will be a problem. Let's get rid of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# a function to get rid of datelines at the start of articles\n",
    "#  matches one or more hyphens or colons in first 40 chars,\n",
    "#  drops everything before that match (plus the match itself)\n",
    "pattern = '[-:]+ '\n",
    "matcher = re.compile(pattern) # compiled regexs are faster\n",
    "\n",
    "def remove_dateline(text, matcher=matcher):\n",
    "    '''\n",
    "    Remove source names and datelines from a text string\n",
    "    If there is a hyphen or colon in the first 40 characters, \n",
    "      drops everything before the hyphen(s)/colon(s)\n",
    "    If no hyphen/colon, do nothing\n",
    "    Return processed string\n",
    "    '''\n",
    "    result = matcher.search(text, endpos=40)\n",
    "    if result:\n",
    "        return text[result.end():]\n",
    "    else:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clean article text\n",
    "news['body'] = news['body'].apply(remove_dateline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sci_Tech</td>\n",
       "      <td>Mobile phone giant Nokia has unveiled its firs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sports</td>\n",
       "      <td>Australia crushed New Zealand by 213 runs on t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sports</td>\n",
       "      <td>Flinching in the trenches, holding around quar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sports</td>\n",
       "      <td>Miami Dolphins owner Wayne Huizenga and presid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sci_Tech</td>\n",
       "      <td>PeopleSoft's Board of Directors voted Wednesda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sci_Tech</td>\n",
       "      <td>Edward Bitet fought in World War II, built aff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Business</td>\n",
       "      <td>LARRY Ellison, the chief executive of software...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sci_Tech</td>\n",
       "      <td>Indonesian police on Tuesday summoned the Amer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>World</td>\n",
       "      <td>Police defused a time-bomb in a town near Prim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>World</td>\n",
       "      <td>A full season has passed since the UN Security...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Sci_Tech</td>\n",
       "      <td>The addictive brutality and enveloping strateg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Sports</td>\n",
       "      <td>Gerald Williams and Eric Valent hit solo home ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>World</td>\n",
       "      <td>The Palestinians have taken a double hit this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Sci_Tech</td>\n",
       "      <td>Hitachi Global Storage Technologies Inc. and I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Sci_Tech</td>\n",
       "      <td>The new desktops, priced between \\$600 and \\$7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Sports</td>\n",
       "      <td>It was a fight that was to receive national ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Business</td>\n",
       "      <td>Kraft Foods Inc. &amp;lt;A HREF=\"http://www.invest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Sports</td>\n",
       "      <td>Jordan Palmer threw a 25-yard touchdown pass t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Sci_Tech</td>\n",
       "      <td>The Open Source Development Labs has gone into...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Sports</td>\n",
       "      <td>Jeff Brehaut shot an 11-under 61 Friday and to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                               body\n",
       "0   Sci_Tech  Mobile phone giant Nokia has unveiled its firs...\n",
       "1     Sports  Australia crushed New Zealand by 213 runs on t...\n",
       "2     Sports  Flinching in the trenches, holding around quar...\n",
       "3     Sports  Miami Dolphins owner Wayne Huizenga and presid...\n",
       "4   Sci_Tech  PeopleSoft's Board of Directors voted Wednesda...\n",
       "5   Sci_Tech  Edward Bitet fought in World War II, built aff...\n",
       "6   Business  LARRY Ellison, the chief executive of software...\n",
       "7   Sci_Tech  Indonesian police on Tuesday summoned the Amer...\n",
       "8      World  Police defused a time-bomb in a town near Prim...\n",
       "9      World  A full season has passed since the UN Security...\n",
       "10  Sci_Tech  The addictive brutality and enveloping strateg...\n",
       "11    Sports  Gerald Williams and Eric Valent hit solo home ...\n",
       "12     World  The Palestinians have taken a double hit this ...\n",
       "13  Sci_Tech  Hitachi Global Storage Technologies Inc. and I...\n",
       "14  Sci_Tech  The new desktops, priced between \\$600 and \\$7...\n",
       "15    Sports  It was a fight that was to receive national ex...\n",
       "16  Business  Kraft Foods Inc. &lt;A HREF=\"http://www.invest...\n",
       "17    Sports  Jordan Palmer threw a 25-yard touchdown pass t...\n",
       "18  Sci_Tech  The Open Source Development Labs has gone into...\n",
       "19    Sports  Jeff Brehaut shot an 11-under 61 Friday and to..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm articles are cleaned\n",
    "news.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute token counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127600, 3486)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up a vectorizer object\n",
    "count_vectorizer = CountVectorizer(\n",
    "    strip_accents='unicode', # collapse accents to base character\n",
    "    stop_words=None, # do not remove stop words\n",
    "    binary=False, # do not binarize features\n",
    "    #max_features=1000,\n",
    "    min_df=0.001 # limit features to those that occur in at least 0.1% of articles\n",
    ")\n",
    "\n",
    "# perform vectorization\n",
    "X = count_vectorizer.fit_transform(news['body'])\n",
    "\n",
    "# vectorized shape\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['amd', 'america', 'american', 'americans', 'amid', 'among',\n",
       "       'amount', 'amp', 'an', 'anaheim'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what do the columns measure?\n",
    "count_vectorizer.get_feature_names_out()[200:210]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3216050"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total word count in our corpus?\n",
    "X.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This corpus isn't very big. Or, well, the documents are pretty short, on average: 3.2M words / 127k documents = about 25 words per document. This fact will make classification a little harder than it would be if we had full documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify sports (vs everything else)\n",
    "\n",
    "We'll try **Bernouli naïve Bayes**, **Multinomial naïve Bayes**, and **Gaussian naïve Bayes** classifiers. The difference is that the Bernouli version first transforms our input features into ones and zeros, thereby coding for the presence or absence of each word in each document (but not accounting for how many times the word occurs in the document). Multinomial NB does not binarize inputs, so accounts for the number of occurrences. But Multinomial NB *does* expect nonnegative integers as input. Gaussian NB works with continuous inputs.\n",
    "\n",
    "There's no principled reason to prefer Bernouli, multinomial, or Gaussian NB. Use whichever one produces better results on your data (and is suitable for the type of input data you're using). Bernouli can be useful in the case where most documents use any word only a few times at most, rare documents use some words very many times, but you don't consider the rare, high-usage documents to be fundamentally unlike the other documents that use the same words just a few times.\n",
    "\n",
    "**Remember:** when we call `.fit()` (as we do explicitly here, or under the hood of `cross_val_score`, for example), what we're learning are the empirical probabilities of each word in each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Sci_Tech', 'Sports', 'Business', 'World'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "Sci_Tech    31900\n",
       "Sports      31900\n",
       "Business    31900\n",
       "World       31900\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make boolean array of sports/other labels\n",
    "y = news['label'] == 'Sports'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127600,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split into train and test sets\n",
    "# --> limit to 1000 articles for demo purposes <--\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[:1000], y[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on TRAINING data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      0.96      0.98       580\n",
      "        True       0.88      0.98      0.93       170\n",
      "\n",
      "    accuracy                           0.97       750\n",
      "   macro avg       0.94      0.97      0.95       750\n",
      "weighted avg       0.97      0.97      0.97       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# score BernouliNB classifier on train and test data\n",
    "clf = BernoulliNB().fit(X_train, y_train)\n",
    "y_train_pred = clf.predict(X_train)\n",
    "print(\"Performance on TRAINING data\")\n",
    "print(classification_report(y_train_pred, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on TEST data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      0.86      0.92       206\n",
      "        True       0.61      0.98      0.75        44\n",
      "\n",
      "    accuracy                           0.88       250\n",
      "   macro avg       0.80      0.92      0.84       250\n",
      "weighted avg       0.93      0.88      0.89       250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = clf.predict(X_test)\n",
    "print(\"Performance on TEST data\")\n",
    "print(classification_report(y_test_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is evidence of overfitting in this case, because we perform significantly better when we make preditions on the training data than when we do the same on held-out test data\n",
    "\n",
    "How about in the multinomial case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on TRAINING data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      1.00      0.99       558\n",
      "        True       0.99      0.97      0.98       192\n",
      "\n",
      "    accuracy                           0.99       750\n",
      "   macro avg       0.99      0.99      0.99       750\n",
      "weighted avg       0.99      0.99      0.99       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# score Multinomial classifier on train and test data\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "y_train_pred = clf.predict(X_train)\n",
    "print(\"Performance on TRAINING data\")\n",
    "print(classification_report(y_train_pred, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on TEST data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      0.96      0.97       181\n",
      "        True       0.90      0.93      0.91        69\n",
      "\n",
      "    accuracy                           0.95       250\n",
      "   macro avg       0.94      0.94      0.94       250\n",
      "weighted avg       0.95      0.95      0.95       250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = clf.predict(X_test)\n",
    "print(\"Performance on TEST data\")\n",
    "print(classification_report(y_test_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinomial NB performs just a tiny bit better in this case, but not enough to matter, really. And we're obviously still overfitting.\n",
    "\n",
    "Aside: One reason to like NB classifiers is that they're really fast. Sklearn's NB classifiers are over 100 times faster than logistic regression on the same task (and they produce comparable accuracy). This equivalent accuracy won't always be true, of course, but the speedup can be a big plus with large datasets.\n",
    "\n",
    "Bernouli NB is slower than multinomial NB due to binarization overhead. But the difference is trivial here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the most informative features\n",
    "\n",
    "Let's see how well we can do with lower-dimension inputs. We'll do this in two ways:\n",
    "\n",
    "1. Rank the input features (token counts) according to the degree to which their presence/absence matches the presence/absence of the `sports` class label. High mutual information between tokens and class labels suggests that a token will be a useful feature for our classifier. We'll also evaluate the *F*-statistic as a criterion and try out permutation importance, too.\n",
    "\n",
    "2. We'll also try dimension reduction via Truncated SVD, as we studied in previous lectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.7 s, sys: 5.58 ms, total: 1.7 s\n",
      "Wall time: 1.7 s\n"
     ]
    }
   ],
   "source": [
    "# calculate mutual info scores btw each feature and the target label\n",
    "%time mi = mutual_info_classif(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not fast!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3486,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00125613, 0.00363214, 0.00069532, ..., 0.0007577 , 0.00152395,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1636,  810, 1762, ..., 1151, 1146, 3485])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get indices of mi array, sorted from highest to lowest mutual info\n",
    "mi_indices = np.flip(mi.argsort()) # np.flip reverses order; want high -> low\n",
    "mi_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discuss**: What do you expect to be some of the most informative features (words) for the **sports** catengory? That is, what words occurs often in sports stories, but not very often in other kinds of news articles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'its'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the single most informative sports feature\n",
    "count_vectorizer.get_feature_names_out()[mi_indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['its', 'cup', 'league', 'game', 'sox', 'red', 'season', 'the',\n",
       "       'victory', 'his'], dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort the feature data and column labels\n",
    "X_train_sorted = X_train[:, mi_indices]\n",
    "X_test_sorted = X_test[:, mi_indices]\n",
    "features_sorted = np.array(count_vectorizer.get_feature_names_out())[mi_indices]\n",
    "features_sorted[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.93      0.90      0.92       186\n",
      "        True       0.73      0.81      0.77        64\n",
      "\n",
      "    accuracy                           0.88       250\n",
      "   macro avg       0.83      0.86      0.84       250\n",
      "weighted avg       0.88      0.88      0.88       250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# performance with top 100 features\n",
    "clf = MultinomialNB().fit(X_train_sorted[:,:100], y_train)\n",
    "y_pred = clf.predict(X_test_sorted[:,:100])\n",
    "print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we've suffered just a few points of weighted average F1 decrease despite throwing away all but 100 out of c. 3,500 original features. (Exact performance will vary with every run, depending on the randomization of the train/test split.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how performance responds in input feature dimensionality in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# how does performance respond to number of features used?\n",
    "num_features = [2**i for i in range(1,10)]\n",
    "f1s = []\n",
    "for i in num_features:\n",
    "    clf = MultinomialNB().fit(X_train_sorted[:,:i], y_train)\n",
    "    y_pred = clf.predict(X_test_sorted[:,:i])\n",
    "    f1s.append(f1_score(y_pred, y_test, average='weighted'))\n",
    "\n",
    "plt.scatter(num_features, f1s)\n",
    "plt.xlabel(\"Number of features used\")\n",
    "plt.ylabel(\"f1 score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In practice\n",
    "\n",
    "Split into train and test, fit a model, predict class labels, and examine classification errors. This is a typical workflow in production.\n",
    "\n",
    "We can make things a little easier by using the `SelectKBest` method, which handles all of the sorting and indexing that we performed by hand above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.93      0.90      0.92       186\n",
      "        True       0.73      0.81      0.77        64\n",
      "\n",
      "    accuracy                           0.88       250\n",
      "   macro avg       0.83      0.86      0.84       250\n",
      "weighted avg       0.88      0.88      0.88       250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using the n most-informative features\n",
    "\n",
    "# fit selector\n",
    "n_features = 100\n",
    "selector = SelectKBest(k=n_features, score_func=mutual_info_classif).fit(X_train, y_train)\n",
    "\n",
    "# select best features\n",
    "X_train_selected = selector.transform(X_train)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "# fit and predict\n",
    "clf = MultinomialNB().fit(X_train_selected, y_train)\n",
    "y_pred = clf.predict(X_test_selected)\n",
    "\n",
    "# examine performance\n",
    "print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: 250\n",
      "Erors: 59\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>Sports</td>\n",
       "      <td>The Seattle Seahawks had a bad fourth quarter against St. Louis two weeks ago, then a bad first quarter last week in New England.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>Sports</td>\n",
       "      <td>Manager Eric Wedge wanted to hear the sound of hammer on rock Monday night at Comerica Park. He wanted to know that the Indians #39; offense was making little ones out of big ones again.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>Sports</td>\n",
       "      <td>Formula One tyre makers Michelin believe high-speed blowouts at the last Belgian Grand Prix and in recent Monza tests were due to sharp kerbs and human error.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>Sports</td>\n",
       "      <td>Tommy Tuberville isn #39;t much into computers and formulas to determine who should play for college football #39;s national championship.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>Sports</td>\n",
       "      <td>Fifth-ranked Florida State was in the process of completing another comeback on the road, and Maryland coach Ralph Friedgen could think of only one course of action.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label  \\\n",
       "255  Sports   \n",
       "938  Sports   \n",
       "827  Sports   \n",
       "345  Sports   \n",
       "810  Sports   \n",
       "\n",
       "                                                                                                                                                                                           body  \n",
       "255  The Seattle Seahawks had a bad fourth quarter against St. Louis two weeks ago, then a bad first quarter last week in New England.                                                           \n",
       "938  Manager Eric Wedge wanted to hear the sound of hammer on rock Monday night at Comerica Park. He wanted to know that the Indians #39; offense was making little ones out of big ones again.  \n",
       "827  Formula One tyre makers Michelin believe high-speed blowouts at the last Belgian Grand Prix and in recent Monza tests were due to sharp kerbs and human error.                              \n",
       "345  Tommy Tuberville isn #39;t much into computers and formulas to determine who should play for college football #39;s national championship.                                                  \n",
       "810  Fifth-ranked Florida State was in the process of completing another comeback on the road, and Maryland coach Ralph Friedgen could think of only one course of action.                       "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# select erroneous predictions\n",
    "errors = y_test != y_pred             # boolean array of gold/pred mismatches\n",
    "errors = np.where(errors==True)       # index array of test set error locations\n",
    "error_index = y_test.index[errors[0]] # map test-set error positions to original indices\n",
    "\n",
    "news_errors = news.loc[error_index, ['label', 'body']] # select errors from original data\n",
    "\n",
    "print(\"Predictions:\", len(y_pred))\n",
    "print(\"Erors:\", len(error_index))\n",
    "\n",
    "with pd.option_context('display.max_colwidth', 0): # display errors\n",
    "    display(news_errors.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Truncated SVD\n",
    "\n",
    "Compare performance using SVD in place of most-informative features.\n",
    "\n",
    "**N.B.** In lecture, we talked about **standardizing** our input features before performing SVD, so that the variance isn't dominated simply by high-frequency words (in this case, words like \"and\" and \"the\" - we didn't perform stopword removal). Here, we're going to skip that step, so that we can preserve input sparsity (hence, computational and memory efficiency).\n",
    "\n",
    "If we *were* going to standardize our features, we'd do something like:\n",
    "\n",
    "```\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X.toarray()) # cast input to dense array\n",
    "```\n",
    "We would need to transform our input matrix to dense format with `.toarray()`, since `StandardScaler` refuses to break sparsity. And if we were going to do all that, we'd be performing PCA anyway, so we'd just use PCA in that case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(750, 100)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduce input dimensionality\n",
    "reducer = TruncatedSVD(n_components=100)\n",
    "X_train_reduced = reducer.fit_transform(X_train)\n",
    "X_test_reduced = reducer.transform(X_test)\n",
    "X_train_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.79085673,  0.70356931,  0.42031277, -0.29701387,  0.64681619,\n",
       "       -0.35532699, -0.15369706,  0.07211047, -0.24273754,  0.30836983,\n",
       "        0.71046793,  0.17294901, -0.30136392, -0.53209643, -0.27567238,\n",
       "       -0.01116163,  0.35941956, -0.01344454,  0.39122922,  0.38794108,\n",
       "        0.04067039, -0.18388449, -0.14394409,  0.24661572, -0.0724706 ,\n",
       "       -0.35682168, -0.22938   ,  0.11162681, -0.027419  , -0.04849846,\n",
       "       -0.19810508,  0.23308084, -0.1643026 ,  0.14313652,  0.35216159,\n",
       "       -0.28014162, -0.33531263,  0.0098925 , -0.24850464,  0.08631965,\n",
       "        0.0733321 , -0.06195463,  0.0088385 , -0.00751238, -0.13254445,\n",
       "        0.08334631, -0.17718607,  0.05814707, -0.04867387,  0.09288686,\n",
       "       -0.01178805, -0.17130755, -0.07392741, -0.18184809,  0.08016494,\n",
       "        0.30478634, -0.08810794, -0.07864459,  0.00770888, -0.08728861,\n",
       "       -0.14103467, -0.22931436, -0.06360749, -0.20461511, -0.00278499,\n",
       "        0.23068106, -0.30556213,  0.08009095, -0.10670783,  0.13427068,\n",
       "       -0.01548563, -0.04256541, -0.09017121, -0.03583415,  0.10979495,\n",
       "        0.04172862, -0.19808212, -0.01600804,  0.1128477 , -0.05581478,\n",
       "        0.28092501, -0.01939768,  0.42720904,  0.15824935,  0.13510167,\n",
       "        0.14148379,  0.1437712 , -0.28871101,  0.03159125, -0.02531832,\n",
       "        0.08710684,  0.07165444, -0.23328475, -0.02373516, -0.27840076,\n",
       "       -0.3353339 , -0.40603342,  0.15433443, -0.04211278, -0.10722505])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine new features for the first document\n",
    "X_train_reduced[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00542199,  0.01055858,  0.00306224, ...,  0.00228302,\n",
       "         0.00113279,  0.        ],\n",
       "       [ 0.00228903,  0.00799538,  0.00564392, ..., -0.00138833,\n",
       "        -0.00180166,  0.        ],\n",
       "       [-0.00251137,  0.00891312,  0.00453846, ..., -0.00118962,\n",
       "        -0.00032591, -0.        ],\n",
       "       ...,\n",
       "       [-0.00129737, -0.06205105, -0.02249646, ...,  0.00581111,\n",
       "         0.00555467,  0.        ],\n",
       "       [-0.0019473 , -0.03590412, -0.01854199, ...,  0.00147461,\n",
       "        -0.00482454,  0.        ],\n",
       "       [-0.01715078,  0.043033  , -0.02233021, ...,  0.01243835,\n",
       "         0.00269987, -0.        ]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# could examine loadings\n",
    "reducer.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3486,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loadings are distributed over all input features\n",
    "reducer.components_[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.98      0.76      0.86       232\n",
      "        True       0.21      0.83      0.34        18\n",
      "\n",
      "    accuracy                           0.76       250\n",
      "   macro avg       0.60      0.80      0.60       250\n",
      "weighted avg       0.93      0.76      0.82       250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# performace using top SVD components\n",
    "clf = GaussianNB().fit(X_train_reduced, y_train)\n",
    "y_pred = clf.predict(X_test_reduced)\n",
    "print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to use `GaussianNB`, because multinomial NB (as the name suggests) expects purely positive, integer-valued inputs. Gaussian NB accepts all real-valued inputs.\n",
    "\n",
    "Performance here isn't as good as it was when we used the top 100 **sports-specific** features (recall that our task is **sports** classification). But the SVD features were more than an order of magnitude faster to calculate and, more importantly, should be useful if we wanted to classify any of the other categories (because the SVD dimensions capture overall variance in the data, rather than being tied to any specific category). \n",
    "\n",
    "As it turns out *in this case*, that isn't really true: top sports features outperform SVD in the sports case, but are surprisingly close to SVD performance for the other categories, too. Think about why this might be true in the special case of language data? (High dimensionality, correlated features, lots of nonuniform structure in the data; 100 features are a *lot* for just 1k observations). If you're interested, you might try to implement these evaluations for yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Permutation importance\n",
    "\n",
    "Permutation importance measures the impact on classifier performance of rendering a feature *uninformative*. The idea is that a feature is important if removing it tanks classification performance. Specifically, we will *permute* (shuffle) the values contained in a feature vector, breaking any possible association between those values and the target label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.02 s, sys: 598 ms, total: 9.62 s\n",
      "Wall time: 8.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = MultinomialNB().fit(X_train_selected, y_train)\n",
    "\n",
    "r = permutation_importance(\n",
    "    clf, \n",
    "    X_test_selected.toarray(), # expects dense input \n",
    "    y_test,\n",
    "    scoring='f1_weighted',\n",
    "    n_repeats=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Permutation feature importance:\n",
      "\n",
      "         cup  0.016 +/- 0.005\n",
      "     england  0.010 +/- 0.005\n",
      "         its  0.010 +/- 0.008\n",
      "        game  0.010 +/- 0.005\n",
      "    williams  0.010 +/- 0.003\n",
      "       right  0.009 +/- 0.003\n",
      "          it  0.008 +/- 0.006\n",
      "     company  0.008 +/- 0.008\n",
      "        said  0.007 +/- 0.006\n",
      "        fans  0.006 +/- 0.003\n",
      "        play  0.006 +/- 0.004\n",
      "     victory  0.006 +/- 0.003\n",
      "      giants  0.005 +/- 0.004\n",
      "         his  0.005 +/- 0.003\n",
      "        shot  0.005 +/- 0.003\n",
      "      league  0.005 +/- 0.002\n",
      "       coach  0.005 +/- 0.003\n",
      "      points  0.005 +/- 0.003\n",
      "    football  0.005 +/- 0.004\n",
      "         tim  0.005 +/- 0.002\n",
      "  postseason  0.004 +/- 0.002\n",
      "     players  0.004 +/- 0.002\n",
      "      rookie  0.004 +/- 0.003\n",
      "        when  0.004 +/- 0.002\n"
     ]
    }
   ],
   "source": [
    "print(\"Permutation feature importance:\\n\")\n",
    "\n",
    "for i in r.importances_mean.argsort()[::-1]:\n",
    "    # only display significant features\n",
    "    if r.importances_mean[i] - 1 * r.importances_std[i] > 0:\n",
    "        word = count_vectorizer.get_feature_names_out()[int(selector.get_feature_names_out()[i].strip('x'))]\n",
    "        print(f\"{word:>12}  {r.importances_mean[i]:.3f} +/- {r.importances_std[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
